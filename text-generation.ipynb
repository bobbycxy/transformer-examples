{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 12:22:20.543739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 12:22:21.075965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os \n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_LEN = 500\n",
    "DECODER_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = BATCH_SIZE*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/transformer-examples/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bobby/code-repo/practices/transformer-examples/env/lib/python3.8/site-packages/datasets/load.py:1454: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"xsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.train_test_split(\n",
    "    train_size=0.1, test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "      <th>train test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sir Iain Lobban will leave later this year, af...</td>\n",
       "      <td>The head of GCHQ - Britain's electronic intell...</td>\n",
       "      <td>25937478</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The bank's quarterly sectoral forecast said im...</td>\n",
       "      <td>The Northern Ireland economy should grow at 2....</td>\n",
       "      <td>27943149</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Developed countries agreed to stop the subsidi...</td>\n",
       "      <td>Countries in the World Trade Organization (WTO...</td>\n",
       "      <td>35145377</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They were trying to explain why the animals ha...</td>\n",
       "      <td>Elephants have enhanced defences against cance...</td>\n",
       "      <td>34466220</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The new one will have 12 sides instead of a sm...</td>\n",
       "      <td>Pocket money may look a bit different this tim...</td>\n",
       "      <td>35935007</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20399</th>\n",
       "      <td>It has advised all international postal servic...</td>\n",
       "      <td>The Republic of Ireland's postal service An Po...</td>\n",
       "      <td>34426129</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20400</th>\n",
       "      <td>It's a kind of all-purpose, good-for-all-occas...</td>\n",
       "      <td>The phrase \"out of touch political elite\" trip...</td>\n",
       "      <td>36562167</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20401</th>\n",
       "      <td>Rachel and Nyomi Fee deny murdering two-year-o...</td>\n",
       "      <td>Evidence of untreated injuries found on Liam F...</td>\n",
       "      <td>36370492</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20402</th>\n",
       "      <td>The move, expected since December, also sees B...</td>\n",
       "      <td>Valtteri Bottas has succeeded retired world ch...</td>\n",
       "      <td>38489054</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>Gary Madine headed in Josh Vela's cross to giv...</td>\n",
       "      <td>Bolton Wanderers thumped Gillingham to move ba...</td>\n",
       "      <td>38204525</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20404 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Sir Iain Lobban will leave later this year, af...   \n",
       "1      The bank's quarterly sectoral forecast said im...   \n",
       "2      Developed countries agreed to stop the subsidi...   \n",
       "3      They were trying to explain why the animals ha...   \n",
       "4      The new one will have 12 sides instead of a sm...   \n",
       "...                                                  ...   \n",
       "20399  It has advised all international postal servic...   \n",
       "20400  It's a kind of all-purpose, good-for-all-occas...   \n",
       "20401  Rachel and Nyomi Fee deny murdering two-year-o...   \n",
       "20402  The move, expected since December, also sees B...   \n",
       "20403  Gary Madine headed in Josh Vela's cross to giv...   \n",
       "\n",
       "                                                 summary        id train test  \n",
       "0      The head of GCHQ - Britain's electronic intell...  25937478      train  \n",
       "1      The Northern Ireland economy should grow at 2....  27943149      train  \n",
       "2      Countries in the World Trade Organization (WTO...  35145377      train  \n",
       "3      Elephants have enhanced defences against cance...  34466220      train  \n",
       "4      Pocket money may look a bit different this tim...  35935007      train  \n",
       "...                                                  ...       ...        ...  \n",
       "20399  The Republic of Ireland's postal service An Po...  34426129      train  \n",
       "20400  The phrase \"out of touch political elite\" trip...  36562167      train  \n",
       "20401  Evidence of untreated injuries found on Liam F...  36370492      train  \n",
       "20402  Valtteri Bottas has succeeded retired world ch...  38489054      train  \n",
       "20403  Bolton Wanderers thumped Gillingham to move ba...  38204525      train  \n",
       "\n",
       "[20404 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(raw_datasets['train'])\n",
    "train_df['train test'] = 'train'\n",
    "\n",
    "df = pd.concat([train_df], axis = 0).reset_index(drop = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = df['document']\n",
    "summary = df['summary']\n",
    "article = article.apply(lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "summary = summary.apply(lambda x: '<SOS> ' + x + ' <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n",
    "    return text\n",
    "article = article.apply(lambda x: preprocess(x))\n",
    "summary = summary.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'\n",
    "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
    "article_tokenizer.fit_on_texts(article)\n",
    "summary_tokenizer.fit_on_texts(summary)\n",
    "inputs = article_tokenizer.texts_to_sequences(article)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114832 27383\n"
     ]
    }
   ],
   "source": [
    "ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\n",
    "DECODER_VOCAB = len(summary_tokenizer.word_index) + 1\n",
    "print(ENCODER_VOCAB, DECODER_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 12:23:34.763485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.763748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.763961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.764165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.772387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.772605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.772801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.772991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.773179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.773364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.773559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:34.773748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.034709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.034978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.035187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.035382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.035574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.035756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.035940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.036122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.036303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.036485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.036663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:35.036842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.215850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.216096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.216302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.216499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.216687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.216873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.217051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.217230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.217433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.217707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20624 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-02-23 12:23:36.218015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.218171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20794 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:82:00.0, compute capability: 8.6\n",
      "2024-02-23 12:23:36.218382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.218535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 20794 MB memory:  -> device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-02-23 12:23:36.218735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-23 12:23:36.218886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 20794 MB memory:  -> device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:c2:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 12:23:40.671777: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 81616000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention \n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8 # number of layers for the encoder and decoder\n",
    "d_model = 256 # dimension of the model\n",
    "dff = 1024 # dimension of the feed-forward network\n",
    "num_heads = 8 # number of heads for the multihead attention \n",
    "dropout_rate = 0.1 # for regularization\n",
    "EPOCHS = 20 # how many times to train over the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=ENCODER_VOCAB,\n",
    "    target_vocab_size=DECODER_VOCAB,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.2151 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 10.0225 Accuracy 0.0334\n",
      "Epoch 1 Batch 200 Loss 9.7061 Accuracy 0.0389\n",
      "Epoch 1 Batch 300 Loss 9.2681 Accuracy 0.0407\n",
      "Epoch 1 Loss 9.1856 Accuracy 0.0409\n",
      "Time taken for 1 epoch: 215.3429934978485 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.6548 Accuracy 0.0410\n",
      "Epoch 2 Batch 100 Loss 7.3920 Accuracy 0.0443\n",
      "Epoch 2 Batch 200 Loss 7.2947 Accuracy 0.0513\n",
      "Epoch 2 Batch 300 Loss 7.2315 Accuracy 0.0566\n",
      "Epoch 2 Loss 7.2205 Accuracy 0.0576\n",
      "Time taken for 1 epoch: 172.02420663833618 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 7.1734 Accuracy 0.0577\n",
      "Epoch 3 Batch 100 Loss 6.8963 Accuracy 0.0648\n",
      "Epoch 3 Batch 200 Loss 6.7879 Accuracy 0.0718\n",
      "Epoch 3 Batch 300 Loss 6.6716 Accuracy 0.0787\n",
      "Epoch 3 Loss 6.6521 Accuracy 0.0799\n",
      "Time taken for 1 epoch: 170.8056001663208 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.4380 Accuracy 0.0800\n",
      "Epoch 4 Batch 100 Loss 6.2153 Accuracy 0.0867\n",
      "Epoch 4 Batch 200 Loss 6.1610 Accuracy 0.0927\n",
      "Epoch 4 Batch 300 Loss 6.1006 Accuracy 0.0985\n",
      "Epoch 4 Loss 6.0897 Accuracy 0.0994\n",
      "Time taken for 1 epoch: 170.61875653266907 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.0254 Accuracy 0.0995\n",
      "Epoch 5 Batch 100 Loss 5.8429 Accuracy 0.1050\n",
      "Epoch 5 Batch 200 Loss 5.8181 Accuracy 0.1097\n",
      "Epoch 5 Batch 300 Loss 5.7677 Accuracy 0.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 13:24:40.249212: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117587968 exceeds 10% of free system memory.\n",
      "2024-02-23 13:24:40.281549: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117587968 exceeds 10% of free system memory.\n",
      "2024-02-23 13:24:40.573843: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117587968 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 5.7600 Accuracy 0.1152\n",
      "Time taken for 1 epoch: 171.7627944946289 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 5.6226 Accuracy 0.1152\n",
      "Epoch 6 Batch 100 Loss 5.5443 Accuracy 0.1196\n",
      "Epoch 6 Batch 200 Loss 5.5128 Accuracy 0.1237\n",
      "Epoch 6 Batch 300 Loss 5.4666 Accuracy 0.1277\n",
      "Epoch 6 Loss 5.4599 Accuracy 0.1283\n",
      "Time taken for 1 epoch: 169.78914713859558 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 5.3799 Accuracy 0.1283\n",
      "Epoch 7 Batch 100 Loss 5.2735 Accuracy 0.1322\n",
      "Epoch 7 Batch 200 Loss 5.2503 Accuracy 0.1356\n",
      "Epoch 7 Batch 300 Loss 5.2103 Accuracy 0.1390\n",
      "Epoch 7 Loss 5.2039 Accuracy 0.1396\n",
      "Time taken for 1 epoch: 169.69656014442444 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 5.0017 Accuracy 0.1397\n",
      "Epoch 8 Batch 100 Loss 5.0361 Accuracy 0.1430\n",
      "Epoch 8 Batch 200 Loss 5.0227 Accuracy 0.1461\n",
      "Epoch 8 Batch 300 Loss 4.9876 Accuracy 0.1491\n",
      "Epoch 8 Loss 4.9814 Accuracy 0.1496\n",
      "Time taken for 1 epoch: 169.61472511291504 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4.7164 Accuracy 0.1497\n",
      "Epoch 9 Batch 100 Loss 4.8335 Accuracy 0.1526\n",
      "Epoch 9 Batch 200 Loss 4.8235 Accuracy 0.1553\n",
      "Epoch 9 Batch 300 Loss 4.7876 Accuracy 0.1581\n",
      "Epoch 9 Loss 4.7826 Accuracy 0.1585\n",
      "Time taken for 1 epoch: 169.35885906219482 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4.6781 Accuracy 0.1585\n",
      "Epoch 10 Batch 100 Loss 4.6481 Accuracy 0.1612\n",
      "Epoch 10 Batch 200 Loss 4.6312 Accuracy 0.1638\n",
      "Epoch 10 Batch 300 Loss 4.6062 Accuracy 0.1663\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 4.5987 Accuracy 0.1667\n",
      "Time taken for 1 epoch: 171.0335555076599 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 4.5030 Accuracy 0.1668\n",
      "Epoch 11 Batch 100 Loss 4.4627 Accuracy 0.1693\n",
      "Epoch 11 Batch 200 Loss 4.4553 Accuracy 0.1716\n",
      "Epoch 11 Batch 300 Loss 4.4303 Accuracy 0.1740\n",
      "Epoch 11 Loss 4.4253 Accuracy 0.1744\n",
      "Time taken for 1 epoch: 169.276433467865 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 4.1822 Accuracy 0.1744\n",
      "Epoch 12 Batch 100 Loss 4.3133 Accuracy 0.1767\n",
      "Epoch 12 Batch 200 Loss 4.3046 Accuracy 0.1789\n",
      "Epoch 12 Batch 300 Loss 4.2792 Accuracy 0.1811\n",
      "Epoch 12 Loss 4.2756 Accuracy 0.1815\n",
      "Time taken for 1 epoch: 169.33918356895447 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 4.4565 Accuracy 0.1815\n",
      "Epoch 13 Batch 100 Loss 4.1705 Accuracy 0.1837\n",
      "Epoch 13 Batch 200 Loss 4.1661 Accuracy 0.1857\n",
      "Epoch 13 Batch 300 Loss 4.1390 Accuracy 0.1878\n",
      "Epoch 13 Loss 4.1323 Accuracy 0.1882\n",
      "Time taken for 1 epoch: 169.42599964141846 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 3.9467 Accuracy 0.1882\n",
      "Epoch 14 Batch 100 Loss 4.0072 Accuracy 0.1904\n",
      "Epoch 14 Batch 200 Loss 3.9909 Accuracy 0.1924\n",
      "Epoch 14 Batch 300 Loss 3.9546 Accuracy 0.1946\n",
      "Epoch 14 Loss 3.9494 Accuracy 0.1950\n",
      "Time taken for 1 epoch: 169.41724181175232 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.0117 Accuracy 0.1950\n",
      "Epoch 15 Batch 100 Loss 3.8135 Accuracy 0.1972\n",
      "Epoch 15 Batch 200 Loss 3.7954 Accuracy 0.1993\n",
      "Epoch 15 Batch 300 Loss 3.7626 Accuracy 0.2015\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 3.7559 Accuracy 0.2019\n",
      "Time taken for 1 epoch: 171.29380464553833 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 3.8136 Accuracy 0.2019\n",
      "Epoch 16 Batch 100 Loss 3.6270 Accuracy 0.2042\n",
      "Epoch 16 Batch 200 Loss 3.6113 Accuracy 0.2064\n",
      "Epoch 16 Batch 300 Loss 3.5826 Accuracy 0.2086\n",
      "Epoch 16 Loss 3.5751 Accuracy 0.2090\n",
      "Time taken for 1 epoch: 169.25587272644043 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 3.6992 Accuracy 0.2091\n",
      "Epoch 17 Batch 100 Loss 3.4536 Accuracy 0.2114\n",
      "Epoch 17 Batch 200 Loss 3.4364 Accuracy 0.2136\n",
      "Epoch 17 Batch 300 Loss 3.4100 Accuracy 0.2160\n",
      "Epoch 17 Loss 3.4027 Accuracy 0.2164\n",
      "Time taken for 1 epoch: 169.44817066192627 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 3.3052 Accuracy 0.2164\n",
      "Epoch 18 Batch 100 Loss 3.2959 Accuracy 0.2188\n",
      "Epoch 18 Batch 200 Loss 3.2768 Accuracy 0.2211\n",
      "Epoch 18 Batch 300 Loss 3.2464 Accuracy 0.2235\n",
      "Epoch 18 Loss 3.2402 Accuracy 0.2240\n",
      "Time taken for 1 epoch: 169.24500250816345 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 3.1385 Accuracy 0.2240\n",
      "Epoch 19 Batch 100 Loss 3.1349 Accuracy 0.2264\n",
      "Epoch 19 Batch 200 Loss 3.1224 Accuracy 0.2288\n",
      "Epoch 19 Batch 300 Loss 3.0963 Accuracy 0.2313\n",
      "Epoch 19 Loss 3.0906 Accuracy 0.2317\n",
      "Time taken for 1 epoch: 169.6409034729004 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 3.1781 Accuracy 0.2318\n",
      "Epoch 20 Batch 100 Loss 2.9966 Accuracy 0.2342\n",
      "Epoch 20 Batch 200 Loss 2.9833 Accuracy 0.2367\n",
      "Epoch 20 Batch 300 Loss 2.9545 Accuracy 0.2392\n",
      "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
      "Epoch 20 Loss 2.9496 Accuracy 0.2397\n",
      "Time taken for 1 epoch: 170.26691150665283 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "   \n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_article):\n",
    "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
    "    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN, \n",
    "                                                                   padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(DECODER_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_article):\n",
    "    summarized = evaluate(input_article=input_article)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  \n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Sir Iain Lobban will leave later this year, after six years as director.\n",
      "The Foreign Office said Sir Iain, 53, was doing \"outstanding job\" and his departure was \"planned\".\n",
      "Officials denied the move was linked to controversy over GCHQ and its US counterpart, the NSA, sparked by disclosures from former US intelligence contractor Edward Snowden.\n",
      "Sir Iain, who first joined GCHQ in 1983, became director in June 2008.\n",
      "\"Today is simply about starting the process of ensuring we have a suitable successor in place before he moves on as planned at the end of the year,\" a Foreign Office spokesman said.\n",
      "ln November, Sir Iain became the first head of the agency to give evidence in public when he appeared before MPs on the Intelligence and Security Committee, alongside the heads of MI5 and MI6.\n",
      "They came under pressure to be more open after leaks by Mr Snowden revealed widespread spying by GCHQ and the US National Security Agency.\n",
      "Sir Iain told the committee Mr Snowden's disclosures had done immense damage to Britain's counter-terrorism efforts.\n",
      "MPs asked Sir Iain why he felt it was necessary to \"collect information on the majority of the public in order to protect us from a minority of evildoers\".\n",
      "He said GCHQ did not spend its time \"listening to the telephone calls or reading the emails of the majority\" of the public. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The head of GCHQ - Britain's electronic intelligence gathering agency - is to step down, the Foreign Office has said. <EOS> \n",
      " Predicted Summary :  the uk is to be the first of the first time in the uk to pay the uk to pay jobs in the uk\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[0],\"\\n Predicted Summary : \", summarize(article[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Sir Iain Lobban will leave later this year, after six years as director.\n",
      "The Foreign Office said Sir Iain, 53, was doing \"outstanding job\" and his departure was \"planned\".\n",
      "Officials denied the move was linked to controversy over GCHQ and its US counterpart, the NSA, sparked by disclosures from former US intelligence contractor Edward Snowden.\n",
      "Sir Iain, who first joined GCHQ in 1983, became director in June 2008.\n",
      "\"Today is simply about starting the process of ensuring we have a suitable successor in place before he moves on as planned at the end of the year,\" a Foreign Office spokesman said.\n",
      "ln November, Sir Iain became the first head of the agency to give evidence in public when he appeared before MPs on the Intelligence and Security Committee, alongside the heads of MI5 and MI6.\n",
      "They came under pressure to be more open after leaks by Mr Snowden revealed widespread spying by GCHQ and the US National Security Agency.\n",
      "Sir Iain told the committee Mr Snowden's disclosures had done immense damage to Britain's counter-terrorism efforts.\n",
      "MPs asked Sir Iain why he felt it was necessary to \"collect information on the majority of the public in order to protect us from a minority of evildoers\".\n",
      "He said GCHQ did not spend its time \"listening to the telephone calls or reading the emails of the majority\" of the public. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The head of GCHQ - Britain's electronic intelligence gathering agency - is to step down, the Foreign Office has said. <EOS> \n",
      " Predicted Summary :  the us has launched a lawsuit on whether the us government to allow its ban to allow gay internet agencies from refugees\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[0],\"\\n Predicted Summary : \", summarize(article[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The bank's quarterly sectoral forecast said improvement in the jobs market and a number of foreign direct investments injected momentum into the recovery.\n",
      "The data was recorded during the second quarter of this year.\n",
      "However, the bank warned that continued austerity could force government departments to reduce staff numbers.\n",
      "Northern Ireland's latest official jobs figures suggest that in the first quarter of this year the private sector added 3,860 jobs while the public sector lost 130 jobs.\n",
      "Danske Bank's report pointed to strong growth in professional and business services like accountancy and recruitment.\n",
      "Its chief economist Angela McGowan said there had been a \"feel-good factor\" in the economy in the second quarter of this year.\n",
      "She said this could be seen in rising levels of household spending and improved levels of business investment.\n",
      "However, she added that households may feel \"a little cheated\" as the economic improvements are still not translating into rising earnings\n",
      "She anticipated that there should be some upward pressure on wages growth in the second half of this year.\n",
      "The overall 2.4% growth figure matches the forecast of the financial consultancy EY, which published its predictions last week.\n",
      "The EY Economic Eye said that consumer and government spending would continue to be the main drivers of growth this year.\n",
      "However, it warned that the reliance on those sectors is unsustainable and that the outlook is slightly less positive for 2015. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The Northern Ireland economy should grow at 2.4% in 2014 and 2015 but 2,400 public sector jobs could be lost in that period, Danske Bank has forecast. <EOS> \n",
      " Predicted Summary :  the number of people killed last year fell on the lowest quarter of the year of the year ending december according to estimates\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[1],\"\\n Predicted Summary : \", summarize(article[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
