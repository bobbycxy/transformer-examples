{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 10:04:59.531903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 10:05:00.050537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os \n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_LEN = 500\n",
    "DECODER_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = BATCH_SIZE*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/transformer-examples/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bobby/code-repo/practices/transformer-examples/env/lib/python3.8/site-packages/datasets/load.py:1454: FutureWarning: The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"xsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.train_test_split(\n",
    "    train_size=0.1, test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "      <th>train test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It happened at about 07:55 BST at Presbar Diec...</td>\n",
       "      <td>A worker has been rescued after being trapped ...</td>\n",
       "      <td>37406267</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Healthcare Improvement Scotland found assessme...</td>\n",
       "      <td>The healthcare watchdog has called for improve...</td>\n",
       "      <td>39503497</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two men went into The Red House Antique Centre...</td>\n",
       "      <td>Police have released CCTV images of two men th...</td>\n",
       "      <td>39818580</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Media playback is unsupported on your device\\n...</td>\n",
       "      <td>These two young pandas didn't fancy taking the...</td>\n",
       "      <td>29861679</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Devon and Cornwall Police said the crash at Pl...</td>\n",
       "      <td>Eighteen people were injured when two trains w...</td>\n",
       "      <td>35954361</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20399</th>\n",
       "      <td>Items from a rare historical collection were t...</td>\n",
       "      <td>Ancient Egyptian artefacts, historical swords,...</td>\n",
       "      <td>37575829</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20400</th>\n",
       "      <td>He stays in charge of the second largest party...</td>\n",
       "      <td>Jeremy Corbyn has been re-elected as the leade...</td>\n",
       "      <td>37460751</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20401</th>\n",
       "      <td>But this time, Egyptian-American journalist Mo...</td>\n",
       "      <td>It was an email like many she had had before: ...</td>\n",
       "      <td>39179674</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20402</th>\n",
       "      <td>Baboucarr Ceesay, from Gambia, is believed to ...</td>\n",
       "      <td>The aunt of a man who is among 800 feared dead...</td>\n",
       "      <td>32402607</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>Stephen Port, 40, of Barking, allegedly met th...</td>\n",
       "      <td>A man accused of poisoning and murdering four ...</td>\n",
       "      <td>35321079</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20404 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      It happened at about 07:55 BST at Presbar Diec...   \n",
       "1      Healthcare Improvement Scotland found assessme...   \n",
       "2      Two men went into The Red House Antique Centre...   \n",
       "3      Media playback is unsupported on your device\\n...   \n",
       "4      Devon and Cornwall Police said the crash at Pl...   \n",
       "...                                                  ...   \n",
       "20399  Items from a rare historical collection were t...   \n",
       "20400  He stays in charge of the second largest party...   \n",
       "20401  But this time, Egyptian-American journalist Mo...   \n",
       "20402  Baboucarr Ceesay, from Gambia, is believed to ...   \n",
       "20403  Stephen Port, 40, of Barking, allegedly met th...   \n",
       "\n",
       "                                                 summary        id train test  \n",
       "0      A worker has been rescued after being trapped ...  37406267      train  \n",
       "1      The healthcare watchdog has called for improve...  39503497      train  \n",
       "2      Police have released CCTV images of two men th...  39818580      train  \n",
       "3      These two young pandas didn't fancy taking the...  29861679      train  \n",
       "4      Eighteen people were injured when two trains w...  35954361      train  \n",
       "...                                                  ...       ...        ...  \n",
       "20399  Ancient Egyptian artefacts, historical swords,...  37575829      train  \n",
       "20400  Jeremy Corbyn has been re-elected as the leade...  37460751      train  \n",
       "20401  It was an email like many she had had before: ...  39179674      train  \n",
       "20402  The aunt of a man who is among 800 feared dead...  32402607      train  \n",
       "20403  A man accused of poisoning and murdering four ...  35321079      train  \n",
       "\n",
       "[20404 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(raw_datasets['train'])\n",
    "train_df['train test'] = 'train'\n",
    "\n",
    "df = pd.concat([train_df], axis = 0).reset_index(drop = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = df['document']\n",
    "summary = df['summary']\n",
    "article = article.apply(lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "summary = summary.apply(lambda x: '<SOS> ' + x + ' <EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n",
    "    return text\n",
    "article = article.apply(lambda x: preprocess(x))\n",
    "summary = summary.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'\n",
    "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
    "article_tokenizer.fit_on_texts(article)\n",
    "summary_tokenizer.fit_on_texts(summary)\n",
    "inputs = article_tokenizer.texts_to_sequences(article)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113226 27610\n"
     ]
    }
   ],
   "source": [
    "ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\n",
    "DECODER_VOCAB = len(summary_tokenizer.word_index) + 1\n",
    "print(ENCODER_VOCAB, DECODER_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 10:05:30.372013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.372330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.372582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.372823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.379419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.379666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.379891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.380112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.380330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.380542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.380750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.380959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.639203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.639440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.639635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.639820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.640859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.641029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:30.641198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.731767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.732996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.733178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.733360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.733540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20624 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-02-27 10:05:31.733908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.734069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20794 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:82:00.0, compute capability: 8.6\n",
      "2024-02-27 10:05:31.734332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.734490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 20794 MB memory:  -> device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-02-27 10:05:31.734764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-27 10:05:31.734924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 20794 MB memory:  -> device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:c2:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention \n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6 # number of layers for the encoder and decoder\n",
    "d_model = 512 # dimension of the model\n",
    "dff = 1024 # dimension of the feed-forward network\n",
    "num_heads = 8 # number of heads for the multihead attention \n",
    "dropout_rate = 0.1 # for regularization\n",
    "EPOCHS = 20 # how many times to train over the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=ENCODER_VOCAB,\n",
    "    target_vocab_size=DECODER_VOCAB,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 10:10:31.440735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-02-27 10:10:31.691913: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f62fc125a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-27 10:10:31.691935: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-02-27 10:10:31.691940: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-02-27 10:10:31.691944: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-02-27 10:10:31.691947: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-02-27 10:10:31.695468: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-27 10:10:31.802235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-02-27 10:10:31.913061: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.2458 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 9.9057 Accuracy 0.0328\n",
      "Epoch 1 Batch 200 Loss 9.4514 Accuracy 0.0455\n",
      "Epoch 1 Batch 300 Loss 8.9133 Accuracy 0.0552\n",
      "Epoch 1 Loss 8.8239 Accuracy 0.0566\n",
      "Time taken for 1 epoch: 256.6080279350281 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.4035 Accuracy 0.0566\n",
      "Epoch 2 Batch 100 Loss 7.1642 Accuracy 0.0645\n",
      "Epoch 2 Batch 200 Loss 7.0776 Accuracy 0.0731\n",
      "Epoch 2 Batch 300 Loss 6.9778 Accuracy 0.0816\n",
      "Epoch 2 Loss 6.9594 Accuracy 0.0831\n",
      "Time taken for 1 epoch: 227.60668206214905 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 6.5758 Accuracy 0.0832\n",
      "Epoch 3 Batch 100 Loss 6.5019 Accuracy 0.0912\n",
      "Epoch 3 Batch 200 Loss 6.3975 Accuracy 0.0984\n",
      "Epoch 3 Batch 300 Loss 6.3024 Accuracy 0.1052\n",
      "Epoch 3 Loss 6.2877 Accuracy 0.1064\n",
      "Time taken for 1 epoch: 227.58877110481262 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.0004 Accuracy 0.1064\n",
      "Epoch 4 Batch 100 Loss 5.9281 Accuracy 0.1127\n",
      "Epoch 4 Batch 200 Loss 5.8565 Accuracy 0.1184\n",
      "Epoch 4 Batch 300 Loss 5.7941 Accuracy 0.1237\n",
      "Epoch 4 Loss 5.7838 Accuracy 0.1247\n",
      "Time taken for 1 epoch: 227.38623642921448 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 5.5842 Accuracy 0.1247\n",
      "Epoch 5 Batch 100 Loss 5.5359 Accuracy 0.1297\n",
      "Epoch 5 Batch 200 Loss 5.4715 Accuracy 0.1345\n",
      "Epoch 5 Batch 300 Loss 5.4239 Accuracy 0.1389\n",
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 5.4165 Accuracy 0.1397\n",
      "Time taken for 1 epoch: 229.1055872440338 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 5.2131 Accuracy 0.1397\n",
      "Epoch 6 Batch 100 Loss 5.2221 Accuracy 0.1439\n",
      "Epoch 6 Batch 200 Loss 5.1636 Accuracy 0.1479\n",
      "Epoch 6 Batch 300 Loss 5.1172 Accuracy 0.1518\n",
      "Epoch 6 Loss 5.1106 Accuracy 0.1525\n",
      "Time taken for 1 epoch: 227.06577038764954 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 4.8966 Accuracy 0.1525\n",
      "Epoch 7 Batch 100 Loss 4.9307 Accuracy 0.1562\n",
      "Epoch 7 Batch 200 Loss 4.8772 Accuracy 0.1599\n",
      "Epoch 7 Batch 300 Loss 4.8392 Accuracy 0.1634\n",
      "Epoch 7 Loss 4.8306 Accuracy 0.1641\n",
      "Time taken for 1 epoch: 227.2039873600006 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 4.6240 Accuracy 0.1641\n",
      "Epoch 8 Batch 100 Loss 4.6694 Accuracy 0.1675\n",
      "Epoch 8 Batch 200 Loss 4.6174 Accuracy 0.1709\n",
      "Epoch 8 Batch 300 Loss 4.5763 Accuracy 0.1742\n",
      "Epoch 8 Loss 4.5712 Accuracy 0.1748\n",
      "Time taken for 1 epoch: 226.97861766815186 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4.3979 Accuracy 0.1749\n",
      "Epoch 9 Batch 100 Loss 4.4135 Accuracy 0.1781\n",
      "Epoch 9 Batch 200 Loss 4.3676 Accuracy 0.1813\n",
      "Epoch 9 Batch 300 Loss 4.3310 Accuracy 0.1845\n",
      "Epoch 9 Loss 4.3244 Accuracy 0.1851\n",
      "Time taken for 1 epoch: 226.70792627334595 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4.1597 Accuracy 0.1851\n",
      "Epoch 10 Batch 100 Loss 4.1831 Accuracy 0.1882\n",
      "Epoch 10 Batch 200 Loss 4.1343 Accuracy 0.1913\n",
      "Epoch 10 Batch 300 Loss 4.0991 Accuracy 0.1944\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 4.0920 Accuracy 0.1950\n",
      "Time taken for 1 epoch: 228.6565866470337 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3.9892 Accuracy 0.1950\n",
      "Epoch 11 Batch 100 Loss 3.9647 Accuracy 0.1980\n",
      "Epoch 11 Batch 200 Loss 3.9191 Accuracy 0.2010\n",
      "Epoch 11 Batch 300 Loss 3.8825 Accuracy 0.2040\n",
      "Epoch 11 Loss 3.8758 Accuracy 0.2045\n",
      "Time taken for 1 epoch: 226.34788060188293 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 3.8775 Accuracy 0.2046\n",
      "Epoch 12 Batch 100 Loss 3.7562 Accuracy 0.2075\n",
      "Epoch 12 Batch 200 Loss 3.7150 Accuracy 0.2104\n",
      "Epoch 12 Batch 300 Loss 3.6839 Accuracy 0.2133\n",
      "Epoch 12 Loss 3.6781 Accuracy 0.2138\n",
      "Time taken for 1 epoch: 226.46639466285706 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 3.6374 Accuracy 0.2138\n",
      "Epoch 13 Batch 100 Loss 3.5656 Accuracy 0.2167\n",
      "Epoch 13 Batch 200 Loss 3.5351 Accuracy 0.2195\n",
      "Epoch 13 Batch 300 Loss 3.5044 Accuracy 0.2223\n",
      "Epoch 13 Loss 3.4974 Accuracy 0.2229\n",
      "Time taken for 1 epoch: 226.41791081428528 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 3.4429 Accuracy 0.2229\n",
      "Epoch 14 Batch 100 Loss 3.3623 Accuracy 0.2257\n",
      "Epoch 14 Batch 200 Loss 3.3075 Accuracy 0.2287\n",
      "Epoch 14 Batch 300 Loss 3.2662 Accuracy 0.2318\n",
      "Epoch 14 Loss 3.2568 Accuracy 0.2323\n",
      "Time taken for 1 epoch: 226.3008327484131 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 3.2285 Accuracy 0.2324\n",
      "Epoch 15 Batch 100 Loss 3.1045 Accuracy 0.2355\n",
      "Epoch 15 Batch 200 Loss 3.0536 Accuracy 0.2386\n",
      "Epoch 15 Batch 300 Loss 3.0125 Accuracy 0.2419\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 3.0021 Accuracy 0.2426\n",
      "Time taken for 1 epoch: 228.05800366401672 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.9884 Accuracy 0.2426\n",
      "Epoch 16 Batch 100 Loss 2.8614 Accuracy 0.2460\n",
      "Epoch 16 Batch 200 Loss 2.8046 Accuracy 0.2494\n",
      "Epoch 16 Batch 300 Loss 2.7630 Accuracy 0.2530\n",
      "Epoch 16 Loss 2.7543 Accuracy 0.2537\n",
      "Time taken for 1 epoch: 226.14948344230652 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 2.6745 Accuracy 0.2537\n",
      "Epoch 17 Batch 100 Loss 2.6101 Accuracy 0.2573\n",
      "Epoch 17 Batch 200 Loss 2.5632 Accuracy 0.2610\n",
      "Epoch 17 Batch 300 Loss 2.5296 Accuracy 0.2648\n",
      "Epoch 17 Loss 2.5212 Accuracy 0.2655\n",
      "Time taken for 1 epoch: 226.27692317962646 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 2.5930 Accuracy 0.2655\n",
      "Epoch 18 Batch 100 Loss 2.3910 Accuracy 0.2693\n",
      "Epoch 18 Batch 200 Loss 2.3468 Accuracy 0.2732\n",
      "Epoch 18 Batch 300 Loss 2.3123 Accuracy 0.2771\n",
      "Epoch 18 Loss 2.3062 Accuracy 0.2778\n",
      "Time taken for 1 epoch: 226.28918433189392 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 2.3188 Accuracy 0.2779\n",
      "Epoch 19 Batch 100 Loss 2.1907 Accuracy 0.2818\n",
      "Epoch 19 Batch 200 Loss 2.1496 Accuracy 0.2858\n",
      "Epoch 19 Batch 300 Loss 2.1199 Accuracy 0.2899\n",
      "Epoch 19 Loss 2.1127 Accuracy 0.2906\n",
      "Time taken for 1 epoch: 226.07647728919983 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 2.1384 Accuracy 0.2907\n",
      "Epoch 20 Batch 100 Loss 2.0175 Accuracy 0.2947\n",
      "Epoch 20 Batch 200 Loss 1.9786 Accuracy 0.2987\n",
      "Epoch 20 Batch 300 Loss 1.9502 Accuracy 0.3028\n",
      "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
      "Epoch 20 Loss 1.9428 Accuracy 0.3036\n",
      "Time taken for 1 epoch: 228.0693416595459 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "   \n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_article):\n",
    "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
    "    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN, \n",
    "                                                                   padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(DECODER_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_article):\n",
    "    summarized = evaluate(input_article=input_article)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  \n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> It happened at about 07:55 BST at Presbar Diecastings Ltd on Store Street.\n",
      "Greater Manchester Fire and Rescue Service (GMFRS) said firefighters used specialist cutting equipment to free the man.\n",
      "He has been taken to hospital where his condition is described as \"stable\".\n",
      "Presbar Diecastings Ltd is a family-owned company which has produced castings since 1969. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> A worker has been rescued after being trapped between a lorry and metal racking at an industrial site in Manchester. <EOS> \n",
      " Predicted Summary :  a man has been taken to hospital after being trapped inside a wall at a house in leicestershire\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[0],\"\\n Predicted Summary : \", summarize(article[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Healthcare Improvement Scotland found assessments were not always completed accurately and there was a lack of \"person-centred\" care plans.\n",
      "But the watchdog said patients and relatives were complimentary about the care they received.\n",
      "NHS Dumfries and Galloway said the report would \"drive improvements\".\n",
      "Its publication followed an unannounced inspection carried out in January.\n",
      "Senior inspector Ian Smith said: \"During our inspection of Dumfries and Galloway Royal Infirmary we found areas of good practice and areas to improve. Interactions between staff and patients were positive and the majority of patients praised the care they received.\n",
      "\"However NHS Dumfries and Galloway must ensure that all older people, who are being treated in accident and emergency or are admitted to hospital, are assessed within the national standard recommended timescales and that that all documentation is dated, timed and filed correctly.\"\n",
      "He added: \"We will continue to monitor the situation and will follow up on these at future inspections.\"\n",
      "The report highlighted areas for improvement:\n",
      "Areas of good practice picked out by the watchdog were:\n",
      "A spokeswoman for NHS Dumfries and Galloway said: \"NHS Dumfries and Galloway welcomes the report and, following the inspection in January, has used the recommendations to drive improvements.\n",
      "\"I am pleased to note the positive comments from patients and their relatives about their experience of care and am committed to ensuring that Dumfries and Galloway hospitals deliver high quality to all individuals who receive care.\" <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The healthcare watchdog has called for improvements in the way Dumfries and Galloway Royal Infirmary treats older patients. <EOS> \n",
      " Predicted Summary :  a charity has launched a review of the health supply hospital's children's hospital after a long ongoing dispute over its patients\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[1],\"\\n Predicted Summary : \", summarize(article[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Two men went into The Red House Antique Centre in Duncombe Place and forced open a cabinet, stealing a number of valuable gold and silver pieces.\n",
      "North Yorkshire Police said the items were distinctive, unique and of historical importance.\n",
      "They include three pairs of gold earrings, a bracelet and a necklace dating back to the 3rd Century AD.\n",
      "Anyone with information about the theft, which happened at about 09:55 BST on 22 April is asked to contact police. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> Police have released CCTV images of two men they want to trace over the theft of Roman jewellery from a shop in York. <EOS> \n",
      " Predicted Summary :  a man has been arrested over the death of a man found at a house in leicestershire\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[2],\"\\n Predicted Summary : \", summarize(article[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Rosalin Baker, 25, and Jeffrey Wiltshire, 52, had denied murdering 16-week-old Imani in September 2016.\n",
      "The child was found to have multiple injuries including 40 rib fractures, a broken wrist and fractured skull.\n",
      "The pair were acquitted of murder but were told by Judge Nicholas Hilliard to expect a \"substantial sentence\".\n",
      "The Old Bailey heard other passengers on the number 25 bus in Stratford tried to help resuscitate Imani after Baker claimed her baby had suddenly become ill during the journey.\n",
      "A witness told the jury Baker \"seemed very relaxed\" as the drama unfolded and \"she was was not crying, she was not shouting\" in spite of the child being \"cold\".\n",
      "Wiltshire, of Newham, east London, was filmed by on-board CCTV kissing Baker and giving her a thumbs up as she boarded the bus with Imani's lifeless body strapped to her chest.\n",
      "She described her partner as a violent drug addict who had forced her to carry the corpse in a sling in order to \"frame\" her.\n",
      "But Wiltshire, who claimed to have fathered 25 children, denied hurting his \"tiny and beautiful\" daughter or being abusive towards Baker.\n",
      "When asked why he gave Baker a thumbs up when when she boarded the bus, he told jurors it was to tell her to \"stay safe\".\n",
      "Prosecutor Duncan Atkinson QC said Imani, who was on the child protection register, had been attacked three times in the week of her death.\n",
      "He said the infant, who was born prematurely and spent the first six weeks of her life in an incubator, would have been in \"very significant pain and distress\" from injuries caused by her arm being twisted or pulled, her chest being squeezed and being thrown against a hard surface.\n",
      "In the week before her child's death Baker had moved from her mother's home in Colchester, Essex, to Wiltshire's bedsit where the family shared the same bed.\n",
      "Giving evidence, she said when she found Imani dead at the bottom of their bed, Wiltshire had told her \"it's all your fault, I told you to give her up for adoption\".\n",
      "\"The first thing I thought was he's done something to her,\" she told the jury.\n",
      "CPS reviewing lawyer Devi Kharran said despite Imani's \"very serious and painful injuries\" neither parent had sought medical help.\n",
      "\"Instead, her lifeless body was carried onto a crowded London bus to disguise the true circumstances of her death,\" he said.\n",
      "Det Insp John Marriott said it was a \"heart breaking\" case particularly because Imani was a premature baby.\n",
      "The pair will be sentenced on 28 May. The maximum sentence for causing or allowing the death of a child is 14 years. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The parents who disguised the death of their baby by pretending she had died on a London bus have been convicted of causing or allowing her death. <EOS> \n",
      " Predicted Summary :  a woman who killed a woman in a swimming pool has been named as she heard as she took charge for her estimate the authorities have said\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[77],\"\\n Predicted Summary : \", summarize(article[77]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
